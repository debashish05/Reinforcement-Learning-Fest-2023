{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import vowpalwabbit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html dataset for 20 class classifcation problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p RL-Transformer\n",
    "\n",
    "newsgroups= fetch_20newsgroups()\n",
    "\n",
    "all_documents = newsgroups[\"data\"]\n",
    "topic_encoder = LabelEncoder()\n",
    "all_targets_mult = topic_encoder.fit_transform(newsgroups[\"target\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "query = \"I had pizza and pasta\"\n",
    "query_vec = sbert_model.encode([query])[0]\n",
    "#print(query_vec)\n",
    "print(len(query_vec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting Text to VW format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vw_format_vector(text, label=None):\n",
    "    vector = sbert_model.encode([text])[0]\n",
    "    data = str(label or \"\") + \" | \"\n",
    "    for i in vector:\n",
    "        data+=str(i)+\" \"\n",
    "    return data+\"\\n\"\n",
    "\n",
    "def to_vw_format(document, label=None):\n",
    "    return (\n",
    "        str(label or \"\")\n",
    "        + \" |text \"\n",
    "        + \" \".join(re.findall(\"\\w{3,}\", document.lower()))\n",
    "        + \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_WRITE_DATA=\"./RL-Transformer\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Test and Train split and divide convert all the data to vw format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    all_documents[:10000], all_targets_mult[:10000], random_state=7\n",
    ")\n",
    "\n",
    "with open(os.path.join(PATH_TO_WRITE_DATA, \"20news_train_mult.vw\"), \"w\") as vw_train_data:\n",
    "    for text, target in zip(train_documents, train_labels):\n",
    "        vw_train_data.write(to_vw_format_vector(text, target))\n",
    "with open(os.path.join(PATH_TO_WRITE_DATA, \"20news_test_mult.vw\"), \"w\") as vw_test_data:\n",
    "    for text in test_documents:\n",
    "        vw_test_data.write(to_vw_format_vector(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "final_regressor = ./RL-Transformer/20news_model_mult.vw\n",
      "using no cache\n",
      "Reading datafile = ./RL-Transformer/20news_train_mult.vw\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, oaa\n",
      "Input label = MULTICLASS\n",
      "Output pred = MULTICLASS\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "\u001b[32m[info]\u001b[m label 20 found -- labels are now considered 1-indexed.\n",
      "1.000000 1.000000            1            1.0             20              1      769\n",
      "1.000000 1.000000            2            2.0             13             20      769\n",
      "1.000000 1.000000            4            4.0             18             15      769\n",
      "1.000000 1.000000            8            8.0              2             11      769\n",
      "0.937500 0.875000           16           16.0              8             10      769\n",
      "0.937500 0.937500           32           32.0              3              6      769\n",
      "0.968750 1.000000           64           64.0             12             14      769\n",
      "0.968750 0.968750          128          128.0             12              7      769\n",
      "0.964844 0.960938          256          256.0             15              8      769\n",
      "0.955078 0.945312          512          512.0             17             18      769\n",
      "0.954102 0.953125         1024         1024.0              1              2      769\n",
      "0.945801 0.937500         2048         2048.0             17             17      769\n",
      "0.947998 0.950195         4096         4096.0              9              8      769\n",
      "\n",
      "finished run\n",
      "number of examples = 7500\n",
      "weighted example sum = 7500.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.951467\n",
      "total feature number = 5767500\n"
     ]
    }
   ],
   "source": [
    "#!vw -d RL/20news_train.vw  --loss_function hinge -f RL/20news_model.vw\n",
    "!vw --oaa 20 ./RL-Transformer/20news_train_mult.vw -f ./RL-Transformer/20news_model_mult.vw --loss_function=hinge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "only testing\n",
      "predictions = RL-Transformer/20news_test_predictions_mult.txt\n",
      "using no cache\n",
      "Reading datafile = RL-Transformer/20news_test_mult.vw\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 7500\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, oaa\n",
      "Input label = MULTICLASS\n",
      "Output pred = MULTICLASS\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "n.a.     n.a.                1            1.0        unknown             12      769\n",
      "n.a.     n.a.                2            2.0        unknown             19      769\n",
      "n.a.     n.a.                4            4.0        unknown             16      769\n",
      "n.a.     n.a.                8            8.0        unknown             13      769\n",
      "n.a.     n.a.               16           16.0        unknown             13      769\n",
      "n.a.     n.a.               32           32.0        unknown             16      769\n",
      "n.a.     n.a.               64           64.0        unknown             14      769\n",
      "n.a.     n.a.              128          128.0        unknown             11      769\n",
      "n.a.     n.a.              256          256.0        unknown             15      769\n",
      "n.a.     n.a.              512          512.0        unknown              9      769\n",
      "n.a.     n.a.             1024         1024.0        unknown             16      769\n",
      "n.a.     n.a.             2048         2048.0        unknown             12      769\n",
      "\n",
      "finished run\n",
      "number of examples = 2500\n",
      "weighted example sum = 2500.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = n.a.\n",
      "total feature number = 1922500\n"
     ]
    }
   ],
   "source": [
    "!vw -i RL-Transformer/20news_model_mult.vw -t -d RL-Transformer/20news_test_mult.vw -p RL-Transformer/20news_test_predictions_mult.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_WRITE_DATA, \"20news_test_predictions_mult.txt\")) as pred_file:\n",
    "    test_prediction_mult = [float(label) for label in pred_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0492\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_labels, test_prediction_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics 7\n",
      "comp.os.ms-windows.misc 5\n",
      "comp.sys.ibm.pc.hardware 2\n",
      "comp.sys.mac.hardware 4\n",
      "comp.windows.x 9\n",
      "misc.forsale 5\n",
      "rec.autos 1\n",
      "rec.motorcycles 5\n",
      "rec.sport.baseball 3\n",
      "rec.sport.hockey 3\n",
      "sci.crypt 7\n",
      "sci.electronics 17\n",
      "sci.med 2\n",
      "sci.space 2\n",
      "soc.religion.christian 8\n",
      "talk.politics.guns 2\n",
      "talk.politics.mideast 10\n",
      "talk.politics.misc 3\n",
      "talk.religion.misc 1\n"
     ]
    }
   ],
   "source": [
    "M = confusion_matrix(test_labels, test_prediction_mult)\n",
    "for i in np.where(M[0, :] > 0)[0][1:]:\n",
    "    print(newsgroups[\"target_names\"][i], M[0, i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
